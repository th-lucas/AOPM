{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the popular music of the last 65 years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The objective is to get the Billboard Year End Hot 100 singles for each year between 1960 and 2010 included.\n",
    "\n",
    "This data is available on Wikipedia. Therefore, the first task is to extract this data from the Wikipedia pages and write it on several csv files (one per year).\n",
    "\n",
    "The format of the useful Wikipedia URLs is the following:\n",
    "https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_{year}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports - API settings - Constants definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # This package is used for scraping the data from Wikipedia\n",
    "import urllib2\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the starting and ending years \n",
    "start_year = 2015\n",
    "end_year = 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting the data from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creation of a list of integers corresponding to all the years we are interested in\n",
    "years = []\n",
    "for i in range(start_year, end_year + 1):\n",
    "    years.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each year, load the Wikipedia page and store the page contents in the soup variable\n",
    "for year in years:\n",
    "    wiki = \"https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_\" + str(year)\n",
    "    header = {'User-Agent': 'Mozilla/5.0'} #Needed to prevent 403 error on Wikipedia\n",
    "    req = urllib2.Request(wiki,headers=header)\n",
    "    page = urllib2.urlopen(req)\n",
    "    soup = BeautifulSoup(page)\n",
    "    \n",
    "    # The rankings are stored in an html table\n",
    "    table = soup.find(\"table\", { \"class\" : \"wikitable sortable\" })\n",
    "    \n",
    "    # This table will be written in a CSV file\n",
    "    \n",
    "    # Open CSV file\n",
    "    f = open('CSV_data/Billboard-charts/Billboard_Year-End_Hot_100_singles_of_' + str(year) + '.csv', 'w')\n",
    "\n",
    "    try:\n",
    "        # CSV writer\n",
    "        writer = csv.writer(f, quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "        # Table header\n",
    "        header_cells = table.findAll(\"th\", {\"scope\" : \"col\"})\n",
    "        header_row_string = []\n",
    "        for index, header_cell in enumerate(header_cells):\n",
    "            if index == 0:\n",
    "                header_row_string.append(\"Num\")\n",
    "            else:\n",
    "                header_row_string.append(header_cell.find(text=True))\n",
    "        writer.writerow(header_row_string)\n",
    "\n",
    "        # Table contents\n",
    "        for row in table.findAll(\"tr\"):\n",
    "            row_string = []\n",
    "            header_row_cells = row.find(\"th\", {\"scope\" : \"row\"})\n",
    "            if header_row_cells != None:\n",
    "                row_string.append(header_row_cells.find(text=True))\n",
    "                \n",
    "            cells = row.findAll(\"td\")\n",
    "            for cell in cells:\n",
    "                text = \"\".join(cell.findAll(text=True))\n",
    "                if text[0] == '\"' and text[len(text) - 1] == '\"':\n",
    "                    text = text[1:-1]\n",
    "                row_string.append(text.encode('utf-8'))\n",
    "            writer.writerow(row_string)\n",
    "\n",
    "    finally:\n",
    "        # Close file\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
